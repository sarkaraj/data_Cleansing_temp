{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from fbprophet import Prophet\n",
    "from dateutil import parser\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dev2\\\\Desktop\\\\coo1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\dev2\\\\Desktop\\\\coo1')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input\n",
    "dir_name = \"C:\\\\Users\\\\dev2\\\\Desktop\\\\coo1\\\\plots\"\n",
    "#Define minimum test and train days\n",
    "test_points = 2\n",
    "min_train_days = 731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "def iso_year_start(iso_year):\n",
    "    \"The gregorian calendar date of the first day of the given ISO year\"\n",
    "    fourth_jan = datetime.date(iso_year, 1, 4)\n",
    "    delta = datetime.timedelta(fourth_jan.isoweekday() - 1)\n",
    "    return fourth_jan - delta\n",
    "\n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"Gregorian calendar date for the given ISO year, week and day\"\n",
    "    year_start = iso_year_start(iso_year)\n",
    "    return year_start + datetime.timedelta(days=iso_day - 1, weeks=iso_week - 1)\n",
    "\n",
    "\n",
    "def make_date(iso_date_string):\n",
    "    date_tuple = iso_date_string.split(\",\")\n",
    "    iso_year = int(date_tuple[0])\n",
    "    iso_week = int(date_tuple[1])\n",
    "    iso_day = int(date_tuple[2])\n",
    "    return iso_to_gregorian(iso_year=iso_year, iso_week=iso_week, iso_day=iso_day)\n",
    "\n",
    "\n",
    "def weeks_for_year(year, max_week_present, min_week_present):\n",
    "    first_week = date(year, 1, 4).isocalendar()\n",
    "    last_week = date(year, 12, 28).isocalendar()\n",
    "\n",
    "    if (first_week < min_week_present):\n",
    "        first_week = min_week_present\n",
    "        if (max_week_present < last_week):\n",
    "            last_week = max_week_present\n",
    "            return (first_week[1], last_week[1])\n",
    "        else:\n",
    "            return (first_week[1], last_week[1])\n",
    "    else:\n",
    "        if (max_week_present < last_week):\n",
    "            last_week = max_week_present\n",
    "            return (first_week[1], last_week[1])\n",
    "        else:\n",
    "            return (first_week[1], last_week[1])\n",
    "\n",
    "\n",
    "def get_missing_weeks(input_year, existing_weeks, max_week_present, min_week_present):\n",
    "    (_first_week, _total_weeks) = weeks_for_year(input_year, max_week_present, min_week_present)\n",
    "    _weeks_list = set(range(_first_week, _total_weeks + 1))\n",
    "    _existing = set(existing_weeks)\n",
    "    _missing = _weeks_list - _existing\n",
    "    return list(_missing)\n",
    "\n",
    "\n",
    "def get_missing_data(df_grpby_year, grp_min_date, grp_max_date):\n",
    "    final_data_df = pd.DataFrame()\n",
    "    for year, group in df_grpby_year:\n",
    "        missing_data_df_per_grp = pd.DataFrame()\n",
    "        grp_dataset = group.copy()\n",
    "        grp_dataset['week_num'] = grp_dataset['isocalendar'].map(lambda x: x[1])\n",
    "        # print grp_dataset\n",
    "        max_week_present = grp_max_date\n",
    "        min_week_present = grp_min_date\n",
    "        _weeks_missed = get_missing_weeks(year, grp_dataset['week_num'], max_week_present, min_week_present)\n",
    "        _weeks_missed_Series = pd.Series(_weeks_missed).map(str)\n",
    "        d = pd.DataFrame(np.zeros((len(_weeks_missed), 2)), columns=['quantity', 'q_indep_p'])\n",
    "        missing_data_df_per_grp = pd.concat([d, _weeks_missed_Series], axis=1)\n",
    "        missing_data_df_per_grp.columns = ['quantity', 'q_indep_p', 'week_num']\n",
    "        missing_data_df_per_grp['year'] = str(year)\n",
    "        missing_data_df_per_grp['isocalendar'] = (\n",
    "        missing_data_df_per_grp['year'] + \",\" + missing_data_df_per_grp['week_num'] + \",\" + \"4\").map(\n",
    "            lambda x: make_date(x).isocalendar())\n",
    "\n",
    "        final_data_df = pd.concat([final_data_df, missing_data_df_per_grp], axis=0)\n",
    "    # print \"printing final_data_df\"\n",
    "    # print final_data_df\n",
    "    return final_data_df\n",
    "\n",
    "\n",
    "def get_cmplt_missing_data(raw_data):\n",
    "    \"\"\"\n",
    "    This fucntion receives raw data\n",
    "    :param raw_data: grouped dataframe\n",
    "    :return: yet to decide\n",
    "    \"\"\"\n",
    "    raw_data_grp = raw_data.groupby(['customernumber', 'matnr'], as_index=False)\n",
    "    final_data_df_1 = pd.DataFrame()\n",
    "    for name, group in raw_data_grp:\n",
    "        # print name[0]\n",
    "\n",
    "        final_df_2 = group.copy()\n",
    "        # final_df_2['date'] = final_df_2['date'].map(str)\n",
    "        # final_df_2['date_parse'] = pd.to_datetime(final_df_2['date'])\n",
    "        # final_df_2['isocalendar'] = final_df_2['date_parse'].map(lambda x: x.isocalendar())\n",
    "        final_df_2['year'] = final_df_2['date_parse'].map(lambda x: x.year)\n",
    "\n",
    "        group_min_date = final_df_2.ix[final_df_2['date_parse'].argmin()].iloc[6]\n",
    "        group_max_date = final_df_2.ix[final_df_2['date_parse'].argmax()].iloc[6]\n",
    "\n",
    "        # print group_min_date\n",
    "        # print group_max_date\n",
    "        final_df_2.drop(['date', 'date_parse'], axis=1)\n",
    "        raw_data_grp_year = final_df_2[\n",
    "            ['customernumber', 'matnr', 'quantity', 'q_indep_p', 'isocalendar', 'year']].groupby(['year'],\n",
    "                                                                                                 as_index=False)\n",
    "        missing_data = get_missing_data(raw_data_grp_year, group_min_date, group_max_date)\n",
    "        missing_data['matnr'] = name[1]\n",
    "        missing_data['customernumber'] = name[0]\n",
    "        # print missing_data\n",
    "        # print \"******************************************************************\"\n",
    "        # final_data_df_2 = pd.concat([final_data_df_2, missing_data], axis=0)\n",
    "        # print final_data_df_2\n",
    "        # print \"##################################################################\"\n",
    "        final_data_df_1 = pd.concat([final_data_df_1, missing_data], axis=0)\n",
    "\n",
    "    # print \"##################################################################\"\n",
    "    final_data_df_1 = final_data_df_1.drop(['week_num', 'year'], axis=1)\n",
    "    # print final_data_df_1\n",
    "\n",
    "    return final_data_df_1\n",
    "\n",
    "def transform_raw_data(raw_data):\n",
    "    result_df = pd.DataFrame()\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy['customernumber'] = raw_data_copy['customernumber'].map(str)\n",
    "    raw_data_copy['date'] = raw_data_copy['date'].map(str)\n",
    "    raw_data_copy['date_parse'] = pd.to_datetime(raw_data_copy['date'])\n",
    "    raw_data_copy['isocalendar'] = raw_data_copy['date_parse'].map(lambda x: x.isocalendar())\n",
    "\n",
    "    missing_Data = get_cmplt_missing_data(raw_data_copy)\n",
    "    # print \"missing data\"\n",
    "    # print missing_Data\n",
    "\n",
    "    raw_data_copy = raw_data_copy.drop(['date', 'date_parse'], axis=1)\n",
    "    # print \"raw_data_copy\"\n",
    "    # print raw_data_copy\n",
    "\n",
    "    result_df = pd.concat([raw_data_copy, missing_Data], axis=0)\n",
    "    # print \"result_df\"\n",
    "    # print result_df\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def weekly_aggregate(data):\n",
    "    data['year_weekNum'] = data['isocalendar'].map(lambda x: (x[0], x[1]))\n",
    "    data = data.drop(['isocalendar'], axis=1)\n",
    "    data_grp = data.groupby(['customernumber', 'matnr', 'year_weekNum'], as_index=False)[\n",
    "        ['quantity', 'q_indep_p']].sum()\n",
    "    data_grp['dt_week'] = data_grp['year_weekNum'].map(lambda x: iso_to_gregorian(int(x[0]), int(x[1]), 4))\n",
    "\n",
    "    data_grp = data_grp.drop(['year_weekNum'], axis=1)\n",
    "    # print \"data_grp\"\n",
    "    # print data_grp\n",
    "    return data_grp\n",
    "\n",
    "\n",
    "def get_weekly_aggregate(inputfile, outputfile, input_sep=\"\\t\", output_sep=\",\"):\n",
    "    raw_data = pd.read_csv(inputfile, sep=input_sep, header=None,\n",
    "                           names=['customernumber', 'matnr', 'date', 'quantity', 'q_indep_p'])\n",
    "    # print raw_data\n",
    "    dataset_cmplt = transform_raw_data(raw_data)\n",
    "    # print dataset_cmplt\n",
    "    result = weekly_aggregate(dataset_cmplt)\n",
    "#     result.to_csv(outputfile, sep=output_sep, index=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_outlier(value, p25, p75):\n",
    "    \"\"\"Check if value is an outlier\n",
    "    \"\"\"\n",
    "    lower = p25 - 2 * (p75 - p25)\n",
    "    upper = p75 + 2 * (p75 - p25)\n",
    "    return value <= lower or value >= upper\n",
    " \n",
    " \n",
    "def get_indices_of_outliers(values):\n",
    "    \"\"\"Get outlier indices (if any)\n",
    "    \"\"\"\n",
    "    p25 = np.percentile(values, 25)\n",
    "    p75 = np.percentile(values, 75)\n",
    "     \n",
    "    indices_of_outliers = []\n",
    "    for ind, value in enumerate(values):\n",
    "        if is_outlier(value, p25, p75):\n",
    "            indices_of_outliers.append(ind)\n",
    "    return indices_of_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>matnr</th>\n",
       "      <th>quantity</th>\n",
       "      <th>q_indep_p</th>\n",
       "      <th>dt_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500008686</td>\n",
       "      <td>100278</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>2015-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500008686</td>\n",
       "      <td>100278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500008686</td>\n",
       "      <td>100278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500008686</td>\n",
       "      <td>100278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500008686</td>\n",
       "      <td>100278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2015-03-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customernumber   matnr  quantity  q_indep_p    dt_week\n",
       "0      500008686  100278      12.0   1.846154 2015-02-05\n",
       "1      500008686  100278       0.0   0.000000 2015-02-12\n",
       "2      500008686  100278       0.0   0.000000 2015-02-19\n",
       "3      500008686  100278       0.0   0.000000 2015-02-26\n",
       "4      500008686  100278       0.0   0.000000 2015-03-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data transformation\n",
    "data = get_weekly_aggregate(inputfile=\"c001_data_52.txt\", outputfile=\"c001_data_52_agg.txt\")\n",
    "data.dt_week = data.dt_week.apply(str).apply(parser.parse)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 0, 0)x(1, 1, 0, 52)12 - AIC:-12.524235893812964\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 0, 1)x(1, 1, 0, 52)12 - AIC:-0.9078664892756692\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 0, 0)x(1, 1, 0, 52)12 - AIC:17.781842670492587\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:33.0702554145123\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:49.899265618970446\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:60.349434009304005\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 0, 0)x(1, 1, 0, 52)12 - AIC:73.40015375823569\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 0, 0)x(1, 1, 0, 52)12 - AIC:83.45664744554787\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:94.90071975144548\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 0, 0)x(1, 1, 0, 52)12 - AIC:-15.90428575846187\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 0, 1)x(1, 1, 0, 52)12 - AIC:-6.098022230424782\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:-11.048955836449824\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:12.51295800876554\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:14.569070951649085\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:18.01057277283087\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:25.155603524733706\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(0, 1, 0)x(1, 1, 0, 52)12 - AIC:30.373869593185177\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:36.610649413209956\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:45.26835932406517\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:49.105159912131754\n",
      "Next Test Starts...\n",
      "Optimal ARIMA(1, 1, 0)x(1, 1, 0, 52)12 - AIC:51.660350433508455\n",
      "Next Test Starts...\n"
     ]
    }
   ],
   "source": [
    "#single product data\n",
    "for cus_no in data.customernumber.unique():\n",
    "    cus_data = data.loc[(data['customernumber'] == cus_no)]\n",
    "    for mat_no in cus_data.matnr.unique():\n",
    "        prod = cus_data[cus_data.matnr == mat_no]\n",
    "\n",
    "        prod = prod.sort_values('dt_week')\n",
    "        prod = prod.reset_index()\n",
    "        prod = prod.rename(columns={'dt_week': 'ds', 'quantity': 'y'})\n",
    "        prod = prod[['ds','y']]\n",
    "        prod = prod.drop(prod.index[[0,len(prod.y)-1]]).reset_index(drop = True)\n",
    "        # prod['y']=prod['y'].replace(0,prod['y'].mean())\n",
    "#         prod.dtypes\n",
    "\n",
    "        # Incremental Test\n",
    "        train = prod[prod.ds <= (max(prod.ds) - pd.DateOffset(days=(max(prod.ds)-min(prod.ds)).days-min_train_days))]\n",
    "        test = prod[(max(train.index)+1):(max(train.index)+1+test_points)]\n",
    "        rem_data = prod[(max(train.index)+test_points):]\n",
    "        output_result = pd.DataFrame()\n",
    "\n",
    "        indices_of_outliers = get_indices_of_outliers(prod.y)\n",
    "\n",
    "        fig = plt.figure()\n",
    "#         ax = fig.add_subplot(111)\n",
    "        plt.plot(prod.ds,prod.y, 'b-', label='Quantity')\n",
    "        plt.plot(\n",
    "            prod.ds[indices_of_outliers],\n",
    "            prod.y[indices_of_outliers],\n",
    "            'ro',\n",
    "            markersize = 7,\n",
    "            label='outliers')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        save_file = os.path.join(dir_name,str(cus_no)+\"_\"+str(mat_no)+\"_raw_data.png\") \n",
    "\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # remove outlier\n",
    "        prod.loc[indices_of_outliers,'y'] = None\n",
    "\n",
    "        #plotting Data\n",
    "        fig = plt.figure()\n",
    "        plt.plot(prod.ds,prod.y)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Quantity')\n",
    "        plt.legend()\n",
    "\n",
    "        save_file = os.path.join(dir_name,str(cus_no)+\"_\"+str(mat_no)+\"_clean_data.png\") \n",
    "\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        while(len(rem_data.ds) >= 2):\n",
    "\n",
    "            #ARIMA Model Data Transform\n",
    "            train_arima = train.set_index('ds',drop=True)\n",
    "            # train_arima.head()\n",
    "            test_arima = test.set_index('ds',drop= True)\n",
    "\n",
    "            # ARIMA Model\n",
    "            #grid search parameters\n",
    "            p = d = q = range(0, 2)\n",
    "\n",
    "            # Generate all different combinations of p, q and q triplets\n",
    "            pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "            # Generate all different combinations of seasonal p, q and q triplets\n",
    "            seasonal_pdq = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "            print('Next Test Starts...')\n",
    "\n",
    "            # grid search\n",
    "            warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "            min_aic = 9999999\n",
    "            for param in pdq:\n",
    "                for param_seasonal in seasonal_pdq:\n",
    "                    try:\n",
    "                        mod = sm.tsa.statespace.SARIMAX(train_arima,order=param,seasonal_order=param_seasonal,\n",
    "                                                        enforce_stationarity=False,enforce_invertibility=False,measurement_error= True)\n",
    "\n",
    "                        results = mod.fit()\n",
    "                        if results.aic < min_aic:\n",
    "                            min_aic = results.aic\n",
    "                            opt_param = param\n",
    "                            opt_param_seasonal = param_seasonal\n",
    "\n",
    "#                         print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "                    except:\n",
    "                        continue\n",
    "            print('Optimal ARIMA{}x{}12 - AIC:{}'.format(opt_param, opt_param_seasonal, min_aic))\n",
    "\n",
    "            #fitting Model\n",
    "            mod = sm.tsa.statespace.SARIMAX(train_arima,order=opt_param,seasonal_order=opt_param_seasonal,\n",
    "                                            enforce_stationarity=False,enforce_invertibility=False,measurement_error= True)\n",
    "            result = mod.fit(disp=False)\n",
    "\n",
    "            # forecast Train\n",
    "            pred_train = results.get_prediction(start=pd.to_datetime(min(train_arima.index)), dynamic=False)\n",
    "            pred_train_ci = pred_train.conf_int()\n",
    "\n",
    "            # forecast test\n",
    "            pred_test = results.get_prediction(start=pd.to_datetime(max(train_arima.index)),end=pd.to_datetime(max(test_arima.index)), dynamic=True)\n",
    "            pred_test_ci = pred_test.conf_int()\n",
    "\n",
    "            # ceating test and train emsembled result  \n",
    "            #test result\n",
    "            result_test = test\n",
    "            result_test['y_ARIMA'] = np.array(pred_test.predicted_mean)[1:]\n",
    "\n",
    "            # prophet\n",
    "            m = Prophet(weekly_seasonality=False,yearly_seasonality=False,changepoint_prior_scale=5)\n",
    "            m.fit(train);\n",
    "\n",
    "            #creating pred train and test data frame \n",
    "            past = m.make_future_dataframe(periods=0, freq= 'W')\n",
    "            future = pd.DataFrame(test['ds'])\n",
    "            pf_train_pred = m.predict(past)\n",
    "            pf_test_pred = m.predict(future)\n",
    "            pf_train_pred = pf_train_pred[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].set_index([past.index])\n",
    "            pf_test_pred = pf_test_pred[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].set_index([future.index])\n",
    "\n",
    "            # ceating test and train emsembled result  \n",
    "            #test result\n",
    "            result_test['y_Prophet'] = np.array(pf_test_pred.yhat)\n",
    "\n",
    "            #Ansemble \n",
    "            result_test['y_Ensembled'] = result_test[[\"y_ARIMA\", \"y_Prophet\"]].mean(axis=1)\n",
    "\n",
    "            train = prod[:(max(train.index)+1+test_points)]\n",
    "            test = prod[(max(train.index)+1):(max(train.index)+1+test_points)]\n",
    "            rem_data = prod[(max(train.index)+test_points):]\n",
    "\n",
    "            output_result = pd.concat([output_result,result_test], axis=0)\n",
    "            output_result['Error'] = np.subtract(output_result.y_Ensembled,output_result.y)\n",
    "            output_result['Error_Cumcum'] = output_result.Error.cumsum()/output_result.y.cumsum()*100\n",
    "\n",
    "        # Plot incremental test result\n",
    "        fig = plt.figure()\n",
    "        plt.plot(output_result.ds,output_result.y, label = 'Observed_test')\n",
    "        plt.plot(output_result.ds,output_result.y_ARIMA, label = 'ARIMA')\n",
    "        plt.plot(output_result.ds,output_result.y_Prophet, label = 'Prophet')\n",
    "        plt.plot(output_result.ds,output_result.y_Ensembled, label = 'Ensembled')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Quantity')\n",
    "        plt.legend()\n",
    "        save_file = os.path.join(dir_name,str(cus_no)+\"_\"+str(mat_no)+\"_prediction.png\") \n",
    "\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        #plot error\n",
    "        fig = plt.figure()\n",
    "\n",
    "        output_result['Error'] = np.subtract(output_result.y_Ensembled,output_result.y)\n",
    "        output_result['Error_Cumsum'] = output_result.Error.cumsum()/output_result.y.cumsum()*100\n",
    "\n",
    "        output_result['Error_prophet'] = np.subtract(output_result.y_Prophet,output_result.y)\n",
    "        output_result['Error_Cumsum_prophet'] = output_result.Error_prophet.cumsum()/output_result.y.cumsum()*100\n",
    "\n",
    "        output_result['Error_arima'] = np.subtract(output_result.y_ARIMA,output_result.y)\n",
    "        output_result['Error_Cumsum_arima'] = output_result.Error_arima.cumsum()/output_result.y.cumsum()*100\n",
    "\n",
    "        plt.plot(output_result.ds[2:], output_result.Error_Cumsum_arima[2:], label = 'ARIMA')\n",
    "        plt.plot(output_result.ds[2:], output_result.Error_Cumsum_prophet[2:], label = 'Prophet')\n",
    "        plt.plot(output_result.ds[2:], output_result.Error_Cumsum[2:], label = 'Ensembled')\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('% Cumulative Error')\n",
    "        plt.legend()\n",
    "\n",
    "        save_file = os.path.join(dir_name,str(cus_no)+\"_\"+str(mat_no)+\"_cum_error.png\") \n",
    "\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Error Arima\n",
    "# abs(sum(output_result.y_ARIMA)-sum(output_result.y))/sum(output_result.y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Error Prophet\n",
    "# abs(sum(output_result.y_Prophet)-sum(output_result.y))/sum(output_result.y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Error Ensembled\n",
    "# abs(sum(output_result.y_Ansembled)-sum(output_result.y))/sum(output_result.y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.matnr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['500008686', '500008702', '500008777', '500008822', '500008847',\n",
       "       '500008848', '500008849', '500008851', '500008853', '500008857',\n",
       "       '500008858', '500008859', '500008861', '500008862', '500008864',\n",
       "       '500008868', '500008876', '500008901', '500008912', '500008921',\n",
       "       '500008952', '500008966', '500008998', '500009000', '500009006',\n",
       "       '500009030', '500009031', '500009032', '500009033', '500009065',\n",
       "       '500009067', '500009117', '500009144', '500009204', '500009223',\n",
       "       '500009225', '500009283', '500009290', '500009305', '500009311',\n",
       "       '500009317', '500009340', '500009346', '500009424', '500009520',\n",
       "       '500009627', '500009657', '500009664', '500009670', '500009707',\n",
       "       '500009744', '500009750', '500009756', '500009877', '500009902',\n",
       "       '500009903', '500009904', '500009908', '500009924', '500009996',\n",
       "       '500009997', '500010003', '500010006', '500010010', '500010013',\n",
       "       '500010015', '500010033', '500010034', '500010035', '500010036',\n",
       "       '500010037', '500010039', '500010040', '500010042', '500010054',\n",
       "       '500010057', '500010058', '500010061', '500010062', '500010066',\n",
       "       '500010067', '500010077', '500010084', '500010090', '500010136',\n",
       "       '500010141', '500010146', '500010160', '500010176', '500010185',\n",
       "       '500010198', '500010202', '500010203', '500010205', '500010207',\n",
       "       '500010239', '500010240', '500010361', '500010379', '500010390',\n",
       "       '500010401', '500010411', '500010446', '500010460', '500010469',\n",
       "       '500010499', '500010552', '500010561', '500010562', '500010565',\n",
       "       '500010572', '500010580', '500010603', '500010670', '500010691',\n",
       "       '500010717', '500010721', '500010728', '500010729', '500010800',\n",
       "       '500010822', '500010826', '500011382', '500095627'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customernumber</th>\n",
       "      <th>matnr</th>\n",
       "      <th>quantity</th>\n",
       "      <th>q_indep_p</th>\n",
       "      <th>dt_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>426928</th>\n",
       "      <td>500095627</td>\n",
       "      <td>100267</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426929</th>\n",
       "      <td>500095627</td>\n",
       "      <td>100267</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426930</th>\n",
       "      <td>500095627</td>\n",
       "      <td>100267</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426931</th>\n",
       "      <td>500095627</td>\n",
       "      <td>100267</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426932</th>\n",
       "      <td>500095627</td>\n",
       "      <td>100267</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-03-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customernumber   matnr  quantity  q_indep_p    dt_week\n",
       "426928      500095627  100267      31.0        NaN 2015-02-26\n",
       "426929      500095627  100267       7.0        NaN 2015-03-05\n",
       "426930      500095627  100267      24.0        NaN 2015-03-12\n",
       "426931      500095627  100267      37.0        NaN 2015-03-19\n",
       "426932      500095627  100267      33.0        NaN 2015-03-26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
